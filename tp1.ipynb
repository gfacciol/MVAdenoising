{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Introduction to CNN denoising\n",
    "\n",
    "\n",
    "The objective of this lesson is to explore some basic aspects of convolutional neuronal networks (CNN) applied to denoising. We will show how to define and train a network in pytorch. As our first network, we are going to implement DCT denoising as a CNN.\n",
    "\n",
    "We will cover the following topics:\n",
    "* Define a simple CNN model inspired on DCT denoising\n",
    "* Apply a CNN to an image\n",
    "* Training a CNN\n",
    "\n",
    "There are **6 questions** in the notebook and corresponding text areas to fill-in the answers. There is also an optinal assignment for extra credit.\n",
    "\n",
    "#### Instructions\n",
    "To solve this TP, answer the questions below. Then export the notebook with the answers using  the menu option **File->Download as->HTML**. Send the resulting *html* file by mail to [facciolo@cmla.ens-cachan.fr](mailto:facciolo@cmla.ens-cachan.fr) with subject \"Report tp1 of SURNAME, Name\", by 26/10/2018.  You will receive an acknowledgement of receipt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code for the notebook\n",
    "\n",
    "# Execute code 'cells' like this by clicking on the 'Run' \n",
    "# button or by pressing [shift] + [Enter].\n",
    "\n",
    "# This cell only imports some python packages that will be\n",
    "# used below. It doesn't generate any output. Something similar \n",
    "# applies to the next two or three cells. They only define \n",
    "# functions that are used later.\n",
    "\n",
    "\n",
    "# This notebook can also run on colab (https://colab.research.google.com/)\n",
    "# The following lines install the necessary packages in the colab environment\n",
    "try:\n",
    "    from google.colab import files\n",
    "    !pip install torch==0.4.1\n",
    "    !pip install torchvision\n",
    "    !pip install Pillow==4.0.0\n",
    "    !pip install scikit-image\n",
    "    !pip install hdf5storage\n",
    "\n",
    "    !pip install git+https://github.com/szagoruyko/pytorchviz\n",
    "\n",
    "    !rm -fr MVAdenoising2018\n",
    "    !git clone  https://github.com/gfacciol/MVAdenoising2018\n",
    "    !cp -r MVAdenoising2018/* .\n",
    "\n",
    "except ImportError:\n",
    "    %matplotlib notebook",
    "    pass\n",
    "\n",
    "\n",
    "# These are all the includes used through the notebook\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io # read and write images\n",
    "import vistools        # image visualization toolbox\n",
    "\n",
    "#%matplotlib notebook\n",
    "# Autoreload external python modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About PyTorch\n",
    "\n",
    "To work with neural networks we will use PyTorch. You can find a very brief review of the main elements that were are going to use [in this notebook](tp1-pytorch-overview.ipynb). If you want to learn more about PyTorch, you can start with the *official* [tutorials](https://pytorch.org/tutorials/) and [documentation](https://pytorch.org/docs/).\n",
    "\n",
    "PyTorch has a series of modules to facilitate defining and training neural networks. The `torch.nn` has a large number of useful classes implementing layers and combination of layers. We will now define a convolutional network with `num_layers` layers and ReLU activations. We encapsulate the definition of the network in a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `test_denoiser` defined below applies a network to an image with simulated noise of standard deviation `sigma`. To apply the `denoiser` network without storing the backpropagation values it must be put in `eval()` mode. To evaluate the result we mesure the Peak Signal to Noise Ratio (PSNR) in decibels (dB) defined as:\n",
    "$$ PSNR=10\\log_{10}\\frac{255^2}{MSE}, $$\n",
    "where $MSE$ is the mean squared error between the denoised image and the noiseless one.\n",
    "\n",
    "The code below calls the `test_denoiser` function on an image from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(img1, img2, peak=255):\n",
    "    '''\n",
    "    Computes the PSNR 'metric' between two images assumed to be in the range [0,1]\n",
    "    '''\n",
    "    x = ((np.array(img1).squeeze() - np.array(img2).squeeze()).flatten() )\n",
    "    return (10*np.log10(peak**2 / np.mean(x**2)))\n",
    "\n",
    "\n",
    "def test_denoiser(denoiser, img_in, sigma=30, show=False, has_noise=False):\n",
    "    '''\n",
    "    Helper function to test a denoising network.\n",
    "\n",
    "    Args:\n",
    "        denoiser: denoising network\n",
    "        img_in: input image (either clean or noisy)\n",
    "        sigma: noise standard deviation\n",
    "        has_noise: set it to True if img_in is a noisy image.\n",
    "                   set it to False if img_in is the clean image. In this\n",
    "                   case noise will be added to test the denoising.\n",
    "        show: if True shows a gallery with the denoising result\n",
    "\n",
    "    Returns:\n",
    "        img_denoised: denoised image\n",
    "        img_noisy: noisy image\n",
    "        psnr_out: psnr after denoising (only if has_noise == False)\n",
    "        psnr_in: psnr before denoising (only if has_noise == False)\n",
    "    '''\n",
    "\n",
    "    # put the image in the range [0,1] and add noise\n",
    "    if has_noise == False:\n",
    "        img_clean = img_in.astype('float32') / 255.\n",
    "        img_test = img_clean + np.random.normal(0, sigma/255.0, img_clean.shape)\n",
    "    else:\n",
    "        img_test = img_in.astype('float32') / 255.\n",
    "\n",
    "    # call the denoiser\n",
    "\n",
    "    # torch data type\n",
    "    dtype = torch.FloatTensor\n",
    "    if torch.cuda.is_available():\n",
    "        # run on GPU\n",
    "        denoiser = denoiser.cuda()\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "\n",
    "    # set denoising network in evaluation (inference) mode\n",
    "    denoiser.eval()\n",
    "\n",
    "    # apply denoising network\n",
    "    with torch.no_grad(): # tell pytorch that we don't need gradients\n",
    "        img = dtype(img_test[np.newaxis,np.newaxis,:,:]) # convert to tensor\n",
    "        out = denoiser.forward(img) # apply network; equivalent to out = denoiser(img)\n",
    "        out = out.cpu() # move to CPU memory\n",
    "\n",
    "    # compute psnr\n",
    "    if has_noise == False:\n",
    "        psnrIN, psnrOUT = PSNR(img_clean, img, 1), PSNR(img_clean, out, 1)\n",
    "    else:\n",
    "        psnrIN, psnrOUT = -1, -1\n",
    "\n",
    "    # scale outputs to [0,255]\n",
    "    out *= 255.\n",
    "    img *= 255.\n",
    "\n",
    "    # visualize as gallery\n",
    "    if show:\n",
    "        if has_noise == False:\n",
    "            vistools.display_gallery([np.array(img_clean).clip(0,1)*255,\n",
    "                                      np.array(img).clip(0,255),\n",
    "                                      np.array(out).clip(0,255)],\n",
    "                                     ['clean', 'noisy (%.2f dB)'%psnrIN,\n",
    "                                      'denoised (%.2f dB)'%psnrOUT])\n",
    "        else:\n",
    "            vistools.display_gallery([np.array(img).clip(0,255),\n",
    "                                      np.array(out).clip(0,255)],                                      \n",
    "                                     ['noisy', 'denoised'])\n",
    "\n",
    "    return out, img, psnrOUT, psnrIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A CNN inspired on DCT denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=400 src=\"http://www.ipol.im/pub/art/2011/ys-dct/revisions/2011-10-24/ys_dct_files/400x-DCT2D.png\"/>\n",
    "\n",
    "*G. Yu, and G. Sapiro, DCT Image Denoising: a Simple and Effective Image Denoising Algorithm, Image Processing On Line, 2011. (https://doi.org/10.5201/ipol.2011.ys-dct)*\n",
    "\n",
    "\n",
    "The DCT denoising algorithm\n",
    "* computes a patch-wise $s \\times s$ DCT transform,\n",
    "* thresholds the coefficients of each patch,\n",
    "* then inverts the DCT transform aggregating the resulting patches.\n",
    "\n",
    "These operations can be implemented in a CNN architecture. The DCT transform of the patches are convolutions (without bias) of the input image with DCT base vectors, which are stored as $s^2$ channels.\n",
    "The inverse transform and aggregation are computed as a convolution with the filter kernels of the inverse DCT base, but rotated $\\pi$ radians (this can be also implemented in pytorch with a **transposed convolution**).\n",
    "\n",
    "The class `DCTlike`, defined below, creates a model with an architecture inspired by DCT denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCTlike(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with an architecture inspired by DCT denosing. It has\n",
    "    two convolutional layers: the first one with s^2, s x s x 1\n",
    "    filters, followed by an activation function and the output\n",
    "    layer with 1 s x s x s^2 filters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ksize=7, sigma=30, initializeDCT=True, shrinkage='hard'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - ksize: patch size for the DCT\n",
    "            - sigma: noise level (multiplies the threshold)\n",
    "            - initializeDCT: if True, initializes the convolutional\n",
    "                layers as the DCT and iDCT transforms; if false it\n",
    "                uses a random initialization.\n",
    "            - shrinkage: type of shrinkage used (hard thresholding, \n",
    "                soft shrinkage or tanh shrinkage)\n",
    "        Returns:\n",
    "            - model: initialized model\n",
    "        \"\"\"\n",
    "        super(DCTlike, self).__init__()\n",
    "        from scipy.fftpack import dct, idct\n",
    "        import numpy as np\n",
    "\n",
    "        dtype = torch.FloatTensor\n",
    "        if torch.cuda.is_available():  dtype = torch.cuda.FloatTensor\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.dct = initializeDCT\n",
    "\n",
    "        ch = ksize**2\n",
    "\n",
    "        # pad by reflection: to have the output with the same size\n",
    "        # as the input we pad the image boundaries. Usually, zero\n",
    "        # padding is used for CNNs. However, since we want to\n",
    "        # reproduce the DCT denoising, we use reflection padding.\n",
    "        # Reflection padding is a differentiable layer.\n",
    "        self.padding = nn.ReflectionPad2d(2*ksize//2-1)\n",
    "\n",
    "        # first convolutional layer (e.g. DCT transform)\n",
    "        self.conv_in = nn.Conv2d(in_channels=1, out_channels=ch,\n",
    "                                 kernel_size=ksize, stride=1,\n",
    "                                 padding=0, bias=not initializeDCT)\n",
    "\n",
    "        # threshold parameter (one variable per frequency)\n",
    "        self.thr = nn.Parameter(dtype(np.ones((1,ch,1,1))),\n",
    "                                requires_grad=True)\n",
    "\n",
    "        # shrinkage function\n",
    "        if   shrinkage == 'hard': self.shrinkage = nn.Hardshrink(1.)\n",
    "        elif shrinkage == 'soft': self.shrinkage = nn.Softshrink(1.)\n",
    "        elif shrinkage == 'tanh': self.shrinkage = nn.Tanhshrink()\n",
    "        else: print('DCTlike: unknown shrinkage option %s' % (shrinkage))\n",
    "\n",
    "        # output conv layer (e.g. inverse DCT transform)\n",
    "        self.conv_out = nn.Conv2d(in_channels=ch, out_channels=1,\n",
    "                                  kernel_size=ksize, stride=1,\n",
    "                                  padding=0, bias=not initializeDCT)\n",
    "\n",
    "        # initialize the isometric DCT transforms\n",
    "        if initializeDCT:\n",
    "\n",
    "            # thresholding parameters (one per feature)\n",
    "            factor = 3.0 if shrinkage == 'hard' else 1.5\n",
    "            thr = np.ones((1,ch,1,1)) * sigma / 255. * factor\n",
    "            thr[0,0] = 1e-3 # don't threshold DC component\n",
    "            self.thr.data = nn.Parameter(dtype(thr), requires_grad=True)\n",
    "\n",
    "            for i in range(ch):\n",
    "                # compute dct coefficients using scipy.fftpack\n",
    "                a=np.zeros((ksize,ksize)); a.flat[i] = 1\n",
    "\n",
    "                # first layer with direct dct transform\n",
    "                a1 = dct(dct(a.T,norm='ortho', type=3).T,norm='ortho', type=3)\n",
    "\n",
    "                self.conv_in.weight.data[i,0,:,:] = nn.Parameter(dtype(a1));\n",
    "\n",
    "                # second layer, inverse transform rotated pi degrees\n",
    "                a2 = idct(idct(a.T,norm='ortho', type=2).T,norm='ortho', type=2)\n",
    "                a2 = np.flip(np.flip(a2, axis=0), axis=1) # pi-rotation\n",
    "\n",
    "                self.conv_out.weight.data[0,i,:,:] = 1/(ch)*nn.Parameter(dtype(a2.copy()))\n",
    "        \n",
    "        # random initialization\n",
    "        else:\n",
    "            # this comes from:\n",
    "            # 1) that the image data follows N(1/2, 1/4) (so 0.5 +- 2*sigma = [0,1])\n",
    "            # 2) imposing the output variance to be 0.5 (the default threshold for\n",
    "            #    hardshrink)\n",
    "            std = 2./np.sqrt(5.)/ksize\n",
    "            for i in range(ch):\n",
    "                self.conv_in .weight.data[i,0,:,:] = dtype(std*np.random.randn(ksize, ksize))\n",
    "                self.conv_out.weight.data[0,i,:,:] = dtype(std*np.random.randn(ksize, ksize))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        out = self.conv_in(self.padding(x))\n",
    "        \n",
    "        # shrinkage non-linearity\n",
    "        if self.dct:\n",
    "            # we use the threshold weights only when using the DCT\n",
    "            out = self.shrinkage(out / self.thr) * self.thr\n",
    "        else:\n",
    "            out = self.shrinkage(out)\n",
    "\n",
    "        # final convolutional layer\n",
    "        out = self.conv_out(out)\n",
    "\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the denoiser with its ideal DCT filters. Compare the results with those of the [IPOL demo of the multi-scale DCT denoising](http://ipolcore.ipol.im/demo/clientApp/demo.html?id=201). This demo computes the result of both the single scale and multiscale DCT denoising.\n",
    "\n",
    "You can also try with random filters by setting `initializeDCT = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import DCTlike\n",
    "from skimage import io\n",
    "\n",
    "sigma=30\n",
    "\n",
    "# creates and initializes a DCTlike network\n",
    "dctnet = DCTlike(8,sigma,initializeDCT=True, shrinkage='hard')\n",
    "\n",
    "# load a clean image\n",
    "img_clean = io.imread('datasets/BSD68/test001.png', dtype='float32')\n",
    "\n",
    "# apply the network and visualize the result\n",
    "_ = test_denoiser(dctnet, img_clean, sigma=sigma, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Compare the results obtained with the `DCTlike` network and single scale DCT results of the IPOL implementation, using the same parameters.  What are the differences between the two methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are trained using large number of examples. In the case of denoising the examples are pairs of noisy (input) and denoised (output) images, which are used to automatically infer rules for denoising the images.\n",
    "For a given pair of noisy and noiseless images $x, y$, the network computes $\\tilde y = \\text{NET}_\\theta(x)$. The training process minimizes the loss $\\ell$ for all the examples:\n",
    "$$ \\min_\\theta \\sum_{(x_i, y_i)\\in \\text{examples}} \\ell( \\text{NET}_\\theta(x_i), y_i)$$\n",
    "\n",
    "For learning several elements are needed:\n",
    "* **data**: a large training set of pairs of noisy and noiseless images\n",
    "* **network model**: the description of the network architecture\n",
    "* **loss function**: the criterion to quantify the error between the denoised and noiseless images\n",
    "* **optimizer**: updates the model parameters (or weights) according to the gradient of the loss. Its hyperparameters control the learning rate and the regularity such as the weight_decay\n",
    "\n",
    "These elements will be the inputs of the training.\n",
    "The training loop consists of an iterative learning process in which the training images are presented to the network in mini-batches. The outputs produced by the network are compared with the corresponding noiseless images according to the loss function.\n",
    "The network weights are adjusted to minimize the loss using the gradient of the loss over the mini-batch, which is computed using backpropagation.\n",
    "After all training images are processed, the process starts over again. Each sweep over the training set is called an **epoch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The training dataset will be handled by a torch `DataLoader`, which combines a dataset and a sampler,\n",
    "and provides iterators over the dataset. For the denoising task the data consists of pairs of noisy/noiseless image patches extracted from a set of images.\n",
    "For simplicity we provide you with a module `denoising_dataloaders` that implements the function `train_validation_denoising_dataloaders` that given a directory containing noiseless images generates the dataloaders for training and validation.\n",
    "\n",
    "```\n",
    "def train_validation_denoising_dataloaders(imagepath,\n",
    "                         noise_sigma=30, crop_size=40,\n",
    "                         train_batch_size=128,\n",
    "                         val_batch_size=32,\n",
    "                         validation_split_fraction=0.1)\n",
    "```\n",
    "\n",
    "The dataloader will randomly crop patches (of size `crop_size`) from the images, apply a random rotation (of an angle multiple of $\\pi/2$), a random reflection and add noise with the specified intensity.  It is important to use two datasets, one for training and another for validation in otder to detect overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's display the data generated by the dataloader\n",
    "\n",
    "For training and validation we will be using the images in `./datasets/Train400/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_dataloaders import train_val_denoising_dataloaders\n",
    "import vistools\n",
    "\n",
    "# create the data loader\n",
    "train_loader, val_loader = \\\n",
    "    train_val_denoising_dataloaders('./datasets/Train400/',\n",
    "                                     noise_sigma=30,\n",
    "                                     crop_size=40,\n",
    "                                     train_batch_size=64)\n",
    "# print size of the mini-batches\n",
    "for i, (X,Y) in enumerate(train_loader):\n",
    "    print('Mini-batch %d containing %d %dx%d crops'\n",
    "         % (i, Y.size(0), Y.size(2), Y.size(3)))\n",
    "\n",
    "    \n",
    "# visualize first mini-batch\n",
    "X, Y = list(train_loader)[0]\n",
    "\n",
    "# this helper function displays the patches in the mini-batch\n",
    "print('\\n\\nA minibatch of noisy patches:')\n",
    "vistools.display_patches(X)\n",
    "\n",
    "print('\\n\\nAnd the corresponding noiseless patches:')\n",
    "vistools.display_patches(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "In the training loop the network processes mini-batches of training images, then compares the resulting outputs against the desired outputs. Errors are then back-propagated through the system for the computation of the gradient of the loss with respect to the parameters of the network. This gradient will be used\n",
    "by an optimizer to adjust the the parameters for the next iteration.\n",
    "\n",
    "To summarize the steps of the learning process:\n",
    "\n",
    "1. *For each epoch*\n",
    "    1. *For each minibatch*\n",
    "        1. *Apply forward model to compute the output*\n",
    "        2. *Compute the loss on the batch*\n",
    "        3. *Backpropagate to compute gradients of the loss w.r.t. the learnable model parameters*\n",
    "        4. *Optimizer updates the model parameters using the gradients computed above*\n",
    "    2. *Each N epochs compute the loss on the validation dataset*\n",
    "\n",
    "\n",
    "The function `trainmodel` defined in the following block implements the above algorithm.\n",
    "\n",
    "\n",
    "We will use the Adam optimizer, which is more sophisticated\n",
    "than the vanila stochastic gradient descent. To learn more about the\n",
    "different optimizers you can refer to [the notes of CS231 course\n",
    "from Stanford University](http://cs231n.github.io/neural-networks-3/#update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Auxiliary function that computes mean of the loss_fn\n",
    "    over the dataset given by dataloader.\n",
    "\n",
    "    Args:\n",
    "        - model: a network\n",
    "        - loss_fn: loss function\n",
    "        - dataloader: the validation data loader\n",
    "\n",
    "    Returns:\n",
    "        - loss over the validation set\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    dtype = torch.FloatTensor\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "        dtype   = torch.cuda.FloatTensor\n",
    "\n",
    "    loss = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in dataloader:\n",
    "\n",
    "            # transform mini-batch to tensors\n",
    "            x_var = x.type(dtype)\n",
    "            y_var = y.type(dtype)\n",
    "\n",
    "            # apply model to x mini-batch\n",
    "            out   = model(x_var)\n",
    "\n",
    "            # accumulate loss\n",
    "            loss += loss_fn(out, y_var)\n",
    "\n",
    "    # return loss divided by number of mini-batches\n",
    "    return loss/len(dataloader)\n",
    "\n",
    "\n",
    "def trainmodel(model, loss_fn, loader_train, loader_val=None,\n",
    "               optimizer=None, scheduler=None, num_epochs = 1,\n",
    "               learning_rate=0.001, weight_decay=0.0, loss_every=10,\n",
    "               save_every=10, filename=None):\n",
    "    \"\"\"\n",
    "    function that trains a network model\n",
    "    Args:\n",
    "        - model       : network to be trained\n",
    "        - loss_fn     : loss functions\n",
    "        - loader_train: dataloader for the training set\n",
    "        - loader_val  : dataloader for the validation set (default None)\n",
    "        - optimizer   : the gradient descent method (default None)\n",
    "        - scheduler   : handles the hyperparameters of the optimizer\n",
    "        - num_epoch   : number of training epochs\n",
    "        - save_every  : save the model every n epochs\n",
    "        - filename    : base filename for the saved models\n",
    "        - loss_every  : print the loss every n epochs\n",
    "        - learning_rate: learning rate (default 0.001)\n",
    "        - weight_decay: weight decay regularization (default 0.0)\n",
    "    Returns:\n",
    "        - model          : trained network\n",
    "        - loss_history   : history of loss values on the training set\n",
    "        - valloss_history: history of loss values on the validation set\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from time import time\n",
    "    import numpy as np\n",
    "\n",
    "    dtype = torch.FloatTensor\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model   = model.cuda()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "        dtype   = torch.cuda.FloatTensor\n",
    "\n",
    "    if optimizer == None or scheduler == None:\n",
    "        # Default optimizer and scheduler\n",
    "\n",
    "        # The optimizer is in charge of updating the parameters\n",
    "        # of the model. It has hyper-parameters for controlling\n",
    "        # the gradient update, such as the learning rate (lr) and\n",
    "        # the regularization such as the weight_decay\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                     betas=(0.9, 0.999), eps=1e-08,\n",
    "                                     weight_decay=weight_decay, amsgrad=False)\n",
    "\n",
    "        # The learning rate scheduler monitors the evolution of the loss\n",
    "        # and adapts the learning rate to avoid plateaus. We will use\n",
    "        # a scheduler available in torch that reduces the lr by 'factor'\n",
    "        # if in the last epochs there hasn't been a significant\n",
    "        # reduction of the validation loss\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "#                            mode='min', factor=0.5, patience=50,\n",
    "                            mode='min', factor=0.8, patience=50,\n",
    "                            verbose=True, threshold=0.0001,\n",
    "                            threshold_mode='rel', cooldown=0,\n",
    "                            min_lr=0, eps=1e-08)\n",
    "\n",
    "    loss_history=[]\n",
    "    valloss_history=[]\n",
    "    \n",
    "    # Display initial training and validation loss\n",
    "    message=''\n",
    "    if loader_val is not None:\n",
    "        valloss = check_accuracy(model, loss_fn, loader_val)\n",
    "        message = ', val_loss = %.4f' % valloss.item()\n",
    "\n",
    "    print('Epoch %5d/%5d, ' % (0, num_epochs) +\n",
    "          'loss = %.4f%s'% (-1, message))\n",
    "\n",
    "    # Save initial results\n",
    "    if filename:\n",
    "        torch.save([model, optimizer, loss_history, valloss_history],\n",
    "                   filename+'%04d.pt' % 0)\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # The data loader iterates once over the whole data set\n",
    "        for (x, y) in loader_train:\n",
    "            # make sure that the models is in train mode\n",
    "            model.train()\n",
    "\n",
    "            # Apply forward model and compute loss on the batch\n",
    "            x = x.type(dtype) # Convert data into pytorch 'variables'\n",
    "            y = y.type(dtype) # for computing the backprop of the loss\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            # Zero out the gradients of parameters that the optimizer\n",
    "            # will update. The optimizer is already linked to the\n",
    "            # network parameters.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backwards pass: compute the gradient of the loss with\n",
    "            # respect to all the learnable parameters of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        # Store loss history to plot it later\n",
    "        loss_history.append(loss)\n",
    "        if loader_val is not None:\n",
    "            valloss = check_accuracy(model, loss_fn, loader_val)\n",
    "            valloss_history.append(valloss)\n",
    "\n",
    "        # Display current loss and compute validation loss\n",
    "        if ((epoch + 1) % loss_every == 0):\n",
    "            message=''\n",
    "            if loader_val is not None:\n",
    "                message = ', val_loss = %.4f' % valloss.item()\n",
    "\n",
    "            print('Epoch %5d/%5d, ' % (epoch + 1, num_epochs) +\n",
    "                  'loss = %.4f%s'% (loss.item(), message))\n",
    "\n",
    "        # Save partial results\n",
    "        if filename and ((epoch + 1) % save_every == 0):\n",
    "            torch.save([model, optimizer, loss_history, valloss_history],\n",
    "                       filename+'%04d.pt' % (epoch + 1))\n",
    "            print('Epoch %5d/%5d, checkpoint saved' % (epoch + 1, num_epochs))\n",
    "\n",
    "        # scheduler update\n",
    "        scheduler.step(loss.data)\n",
    "\n",
    "    # Save last result\n",
    "    if filename:\n",
    "        torch.save([model, optimizer, loss_history, valloss_history],\n",
    "                    filename+'%04d.pt' % (epoch + 1))\n",
    "\n",
    "    return model, loss_history, valloss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the DCT-like denoiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "We will now run a few training iterations on our DCT-like denoiser. Use the following code to train a DCT network with 4x4 kernels with the previous training functions. Try starting initializing the network with and without the DCT basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now train from that initial condition\n",
    "\n",
    "from denoising_dataloaders import train_val_denoising_dataloaders\n",
    "\n",
    "# nose level\n",
    "sigma=30\n",
    "\n",
    "# construct a training and a validation data loader\n",
    "train_loader, val_loader = train_val_denoising_dataloaders(\n",
    "                                           './datasets/Train400/',\n",
    "                                           noise_sigma=sigma,\n",
    "                                           crop_size=40,\n",
    "                                           train_batch_size=180)\n",
    "\n",
    "    \n",
    "# build dct like network with 4x4 patches and soft shrinkage\n",
    "#dctnet = DCTlike(8,sigma,initializeDCT=True, shrinkage='soft')\n",
    "dctnet = DCTlike(4,sigma,initializeDCT=False, shrinkage='soft')\n",
    "    \n",
    "    \n",
    "# define loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# run the training loop\n",
    "#filename = 'trainings/DCT8_soft-thr_L2_dct-init_' # for dct initialization\n",
    "filename = 'trainings/DCT4_soft-thr_L2_rnd-init_' # for random initialization\n",
    "dctnet, tloss, vloss, = trainmodel(dctnet, loss, train_loader, val_loader,\n",
    "                                   num_epochs=200,\n",
    "                                   learning_rate=1e-3,\n",
    "                                   save_every=100, loss_every=10,\n",
    "                                   filename=filename)\n",
    "\n",
    "# plot loss\n",
    "plt.semilogy(vloss, label='val')\n",
    "plt.semilogy(tloss, label='train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result of the training\n",
    "\n",
    "sigma=30\n",
    "\n",
    "# Load the result of the previous trainig\n",
    "dctnet = torch.load('trainings/DCT4_soft-thr_L2_rnd-init_0200.pt')[0]\n",
    "\n",
    "img_clean = io.imread('datasets/BSD68/test001.png', dtype='float32')\n",
    "_ = test_denoiser(dctnet, img_clean, sigma=sigma, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's no surprise that the above network didn't work very well. 100 epochs is not even enough to converge.**\n",
    "\n",
    "Let's compare some training results computed with more computational resources. You will find pretrained DCT networks  with 4x4 and 8x8 kernels, initialized with the DCT kernels or randomly. These networks have been trained on a GPU using mini-batches of 360 40x40 crops. When the initialization was random, we used 8000 epochs and an initial learning rate of 1e-2. When starting from the DCT, we used 1000 epochs and a smaller learning rate 1e-4. Checkpoints were saved every 500 iterations (starting with 0).\n",
    "\n",
    "You can load these checkpoints using `torch.load`. For example:\n",
    "\n",
    "`checkpoint = torch.load('pre-trained/DCT8_soft-thr_L2_rnd-init-1_3500.pt')`\n",
    "\n",
    "This loads the iteration 3500 during training of a DCT-like network with 8x8 kernels (`DCT8`) and soft threshold non-linearity. The initial condition was a random initialization (`rnd-init-1`) and the loss was the squared L2 norm (MSE). The `-1` in `rnd-init-1` means that it was the first attempt. \n",
    "\n",
    "Another example is:\n",
    "\n",
    "`checkpoint = torch.load('pre-trained/DCT4_soft-thr_L2_dct-init_0500.pt')`\n",
    "\n",
    "This loads training iteration 500 of a DCT-like net with 4x4 kernels and soft-shrinkage, initialized with the DCT and using the squared L2 loss.\n",
    "\n",
    "The saved checkpoints are a tuple with the following data:\n",
    "- `checkpoint[0]`: the dct-like network\n",
    "- `checkpoint[1]`: the optimizer used\n",
    "- `checkpoint[2]`: the evolution of the training loss\n",
    "- `checkpoint[3]`: the evolution of the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the evolution of the loss\n",
    "\n",
    "# load last checkpoint\n",
    "dctnet1 = torch.load('pre-trained/DCT8_soft-thr_L2_rnd-init_2000.pt', map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "plt.semilogy(dctnet1[3], label='net1 val')\n",
    "plt.semilogy(dctnet1[2], label='net1 train')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "dctnet2 = torch.load('pre-trained/DCT8_soft-thr_L2_dct-init_2000.pt', map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "plt.semilogy(dctnet2[3], label='net2 val')\n",
    "plt.semilogy(dctnet2[2], label='net2 train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Did the trainings converge to local minima? What can you say about the quality these local minima?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows the evolution of the denoising performance during training\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# load an image\n",
    "img_clean = io.imread('datasets/BSD68/test002.png', dtype='float32')\n",
    "img_noisy = img_clean + np.random.normal(0, sigma, img_clean.shape)\n",
    "\n",
    "outs = []\n",
    "labels = []\n",
    "\n",
    "# add noisy image\n",
    "outs.append(np.array(img_noisy).clip(0,255))\n",
    "labels.append('noisy')\n",
    "\n",
    "# add clean image\n",
    "outs.append(np.array(img_clean).clip(0,255))\n",
    "labels.append('clean')\n",
    "\n",
    "# add results of iterations\n",
    "for i in range(0,2001,200):\n",
    "    net = torch.load('pre-trained/DCT4_soft-thr_L2_rnd-init_%04d.pt' % i, map_location={'cuda:0': 'cpu'})[0]\n",
    "    out = test_denoiser(net, img_noisy, sigma, has_noise=True)[0]\n",
    "    outs.append(np.array(out).clip(0,255))\n",
    "    labels.append('trained dct net - it %d - %f (dB)' % (i, PSNR(out, img_clean)))\n",
    "\n",
    "# add result of original dct\n",
    "original_dct = DCTlike(8, sigma, initializeDCT=True)\n",
    "out = test_denoiser(original_dct, img_noisy, sigma, has_noise=True)[0]\n",
    "outs.append(np.array(out).clip(0,255))\n",
    "labels.append('original dct - %f (dB)' % (PSNR(out, img_clean)))\n",
    "\n",
    "\n",
    "vistools.display_gallery(outs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell compares denoising results of different pre-trained networks\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# load denoising nets\n",
    "\n",
    "# load an image\n",
    "img_clean = io.imread('datasets/BSD68/test002.png', dtype='float32')\n",
    "img_noisy = img_clean + np.random.normal(0, sigma, img_clean.shape)\n",
    "\n",
    "outs = []\n",
    "labels = []\n",
    "\n",
    "# original dct denoising 8x8\n",
    "original_dct = DCTlike(8, sigma, initializeDCT=True)\n",
    "out = test_denoiser(original_dct, img_noisy, sigma, has_noise=True)[0]\n",
    "outs.append(np.array(out).clip(0,255))\n",
    "labels.append('original dct 8x8 - %f (dB)' % (PSNR(out, img_clean)))\n",
    "\n",
    "# trained starting from dct 8x8\n",
    "dctnet1 = torch.load('pre-trained/DCT8_soft-thr_L2_dct-init_1000.pt', map_location={'cuda:0': 'cpu'})[0]\n",
    "out = test_denoiser(dctnet1, img_noisy, sigma, has_noise=True)[0]\n",
    "outs.append(np.array(out).clip(0,255))\n",
    "labels.append('trained from dct 8x8 - %f (dB)' % (PSNR(out, img_clean)))\n",
    "\n",
    "# original dct denoising 4x4\n",
    "original_dct = DCTlike(4, sigma, initializeDCT=True)\n",
    "out = test_denoiser(original_dct, img_noisy, sigma, has_noise=True)[0]\n",
    "outs.append(np.array(out).clip(0,255))\n",
    "labels.append('original dct 4x4 - %f (dB)' % (PSNR(out, img_clean)))\n",
    "\n",
    "# trained starting from dct 4x4\n",
    "dctnet2 = torch.load('pre-trained/DCT4_soft-thr_L2_dct-init_2000.pt', map_location={'cuda:0': 'cpu'})[0]\n",
    "out = test_denoiser(dctnet2, img_noisy, sigma, has_noise=True)[0]\n",
    "outs.append(np.array(out).clip(0,255))\n",
    "labels.append('trained from dct 4x4 - %f (dB)' % (PSNR(out, img_clean)))\n",
    "\n",
    "# ... you can add more to the comparison ...\n",
    "\n",
    "outs.append(np.array(img_clean).clip(0,255))\n",
    "labels.append('clean')\n",
    "\n",
    "outs.append(np.array(img_noisy).clip(0,255))\n",
    "labels.append('noisy')\n",
    "\n",
    "vistools.display_gallery(outs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Give a brief description (a couple of lines) of the qualitative differences between the original DCT and the trained networks initialized with DCT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** What are the receptive fields of the networks using 4x4 and 8x8 filters? How does this impact on the results? \n",
    "\n",
    "*Note: the receptive field is the set of input pixels that affect a single output pixel.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell displays the input and output filters of a network\n",
    "\n",
    "# load a network\n",
    "dctnet = torch.load('pre-trained/DCT8_soft-thr_L2_rnd-init_2000.pt', map_location={'cuda:0': 'cpu'})[0]\n",
    "#dctnet = torch.load('pre-trained/DCT8_soft-thr_L2_dct-init_1000.pt', map_location={'cuda:0': 'cpu'})[0]\n",
    "\n",
    "# Number of filters\n",
    "N = dctnet.conv_in.weight.shape[0]\n",
    "w_in  = [ dctnet.conv_in .weight[i,0,:,:].detach().cpu().numpy() for i in range(N) ]\n",
    "w_out = [ dctnet.conv_out.weight[0,i,:,:].detach().cpu().numpy() for i in range(N) ]\n",
    "\n",
    "print ('INPUT FILTERS')\n",
    "vistools.display_patches(w_in)\n",
    "\n",
    "print ('OUTPUT FILTERS')\n",
    "vistools.display_patches(w_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deeper architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now work with **DnCNN** a deep convolutional network with a simple architecutre introduced by\n",
    "\n",
    "*K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,” IEEE Trans. Image Process., vol. 26, no. 7, pp. 3142–3155, Jul. 2017.*\n",
    "\n",
    "The network has several hidden layers, all of them equal: convolution, **batch normalization** and **ReLU activations** and uses **residual learning**. All convolutions have the same size (the authors used 3x3).\n",
    "\n",
    "<img width=700 src=\"https://www.researchgate.net/profile/Yunjin_Chen/publication/306187437/figure/fig4/AS:667093628379148@1536058923422/The-architecture-of-the-proposed-DnCNN-network.png\"/>\n",
    "\n",
    "\n",
    "The code below declares the DnCNN network. \n",
    "An instance of the network for grayscale images is created with `model = DnCNN(1,1)` where the parameters `1` indicate the number of input and output channels.\n",
    "The model is made of atomic blocks such as `nn.Conv2d`, whch represents a convolutional layer. Note how the layers are declared in the `__init__` method of each model and then called in `forward`. \n",
    "\n",
    "A newly created network is initialized with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONV_BN_RELU(nn.Module):\n",
    "    '''\n",
    "    PyTorch Module grouping together a 2D CONV, BatchNorm and ReLU layers.\n",
    "    This will simplify the definition of the DnCNN network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels=128, out_channels=128, kernel_size=7, \n",
    "                 stride=1, padding=3):\n",
    "        '''\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            - in_channels: number of input channels from precedding layer\n",
    "            - out_channels: number of output channels\n",
    "            - kernel_size: size of conv. kernel\n",
    "            - stride: stride of convolutions\n",
    "            - padding: number of zero padding\n",
    "\n",
    "        Return: initialized module\n",
    "        '''\n",
    "        super(CONV_BN_RELU, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                              stride=stride, padding=padding)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Applies the layer forward to input x\n",
    "        '''\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return(out)\n",
    "\n",
    "\n",
    "\n",
    "class DnCNN(nn.Module):\n",
    "    '''\n",
    "    PyTorch module for the DnCNN network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_layers=17, \n",
    "                 features=64, kernel_size=3, residual=True):\n",
    "        '''\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            - in_channels: input image channels (default 1)\n",
    "            - out_channels: output image channels (default 1)\n",
    "            - num_layers: number of layers (default 17)\n",
    "            - num_features: number of hidden features (default 64)\n",
    "            - kernel_size: size of conv. kernel (default 3)\n",
    "            - residual: use residual learning (default True)\n",
    "\n",
    "        Return: initialized network\n",
    "        '''\n",
    "        super(DnCNN, self).__init__()\n",
    "        \n",
    "        self.residual = residual\n",
    "        \n",
    "        # a list for the layers\n",
    "        self.layers = []  \n",
    "        \n",
    "        # first layer \n",
    "        self.layers.append(CONV_BN_RELU(in_channels=in_channels,\n",
    "                                        out_channels=features,\n",
    "                                        kernel_size=kernel_size,\n",
    "                                        stride=1, padding=kernel_size//2))\n",
    "        # intermediate layers\n",
    "        for _ in range(num_layers-2):\n",
    "            self.layers.append(CONV_BN_RELU(in_channels=features,\n",
    "                                            out_channels=features,\n",
    "                                            kernel_size=kernel_size,\n",
    "                                            stride=1, padding=kernel_size//2))\n",
    "        # last layer \n",
    "        self.layers.append(nn.Conv2d(in_channels=features,\n",
    "                                     out_channels=out_channels,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     stride=1, padding=kernel_size//2))\n",
    "        # chain the layers\n",
    "        self.dncnn = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward operation of the network on input x.'''\n",
    "        out = self.dncnn(x)\n",
    "        \n",
    "        if self.residual: # residual learning\n",
    "            out = x - out \n",
    "        \n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** The following architecture achieves state-of-the-art results (when trained). Count the number of parameters and the size of the receptive field for the network:\n",
    "\n",
    "`m = DnCNN(in_channels=1, out_channels=1, num_layers=17, features=64, kernel_size=3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaser: apply a state-of-the-art network\n",
    "\n",
    "Let's first load a state-of-the-art denoising network, just to see what a deep network is capable of. We will load the network parameters trained by the authors of DnCNN, and test it on an image. Compare it with the result of our best DCT denoising network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DnCNN_pretrained_grayscale\n",
    "\n",
    "# load an image (change the number to test other images)\n",
    "image = io.imread('datasets/BSD68/test002.png', dtype='float32')\n",
    "\n",
    "# set a noise level\n",
    "sigma = 30\n",
    "\n",
    "# add noise\n",
    "im_clean = image.astype('float32')\n",
    "im_noisy = im_clean + np.random.normal(0, sigma, im_clean.shape)\n",
    "\n",
    "# load two versions of DnCNN: random weights and trained weights\n",
    "dncnn_random = DnCNN(in_channels=1, out_channels=1, num_layers=17, features=64, kernel_size=3, residual=True)\n",
    "dncnn_trained = DnCNN_pretrained_grayscale(sigma)\n",
    "\n",
    "# apply them to the image\n",
    "out_random  = test_denoiser(dncnn_random , im_noisy, sigma, has_noise=True)[0]\n",
    "out_trained = test_denoiser(dncnn_trained, im_noisy, sigma, has_noise=True)[0]\n",
    "\n",
    "# compute PSNRs\n",
    "PSNR_noisy   = PSNR(im_noisy   , im_clean)\n",
    "PSNR_random  = PSNR(out_random , im_clean)\n",
    "PSNR_trained = PSNR(out_trained, im_clean)\n",
    "\n",
    "# show as a gallery\n",
    "vistools.display_gallery([np.array(im_noisy   ).clip(0,255),\n",
    "                          np.array(out_random ).clip(0,255),\n",
    "                          np.array(out_trained).clip(0,255),\n",
    "                          np.array(im_clean   ).clip(0,255)],\n",
    "                         ['noisy  (%.2f dB)' % PSNR_noisy,\n",
    "                          'denoised random  weights (%.2f dB)' % PSNR_random,\n",
    "                          'denoised trained weights (%.2f dB)' % PSNR_trained,\n",
    "                          'clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** It is no surprise that DnCNN is much better than DCT, it has a much larger receptive field and number of parameters. What would be the number of layers a DnCNN network using 3x3 kernels so that the receptive field matches that of our DCT-like network with patches of 8x8? For this number of layers, compute the number of features of the hidden layers resulting in a nunber of parameters that roughly matches that of the DCT8 network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit: train a tiny DnCNN\n",
    "\n",
    "As an optional assignment, you will do a fair comparison of the deep  architecture of DnCNN and the shallow DCTlike network using 8x8 patches. For that use the parameters computed in the question above to ensure that both networks have the same receptive field and number of parameters.\n",
    "\n",
    "Train the network using the block below. We recommend running 1000 or 2000 epochs. Run the remaining blocks to observe the result.\n",
    "\n",
    "*Note: You can leave the training running on the server and later connet back to the notebook to reap your results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_dataloaders import train_val_denoising_dataloaders\n",
    "from training import trainmodel\n",
    "\n",
    "\n",
    "sigma=30\n",
    "\n",
    "# data\n",
    "trainloader, validationloader = train_val_denoising_dataloaders(\n",
    "                                          './datasets/Train400/', \n",
    "                                           noise_sigma=sigma, crop_size=40, \n",
    "                                           train_batch_size=45 )\n",
    "\n",
    "# network model TO COMPLETE\n",
    "Denoiser = DnCNN(in_channels=1, out_channels=1, \n",
    "                 num_layers=1,#... [COMPLETE WITH RESULT OF QUESTION 6] ...  \n",
    "                 features=1,  #... [COMPLETE WITH RESULT OF QUESTION 6] ... \n",
    "                 kernel_size=3, \n",
    "                 residual=True)\n",
    "\n",
    "\n",
    "# loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "# run the training loop\n",
    "Denoiser, loss, lossv, = trainmodel(Denoiser, loss, trainloader, validationloader, \n",
    "                                    num_epochs=1000, save_every=250, loss_every=10,  \n",
    "                                    learning_rate=0.01, weight_decay=0.00001,\n",
    "                                    filename='trainings/tiny_DnCNN_')\n",
    "\n",
    "# plot loss\n",
    "plt.semilogy(lossv, label='val')\n",
    "plt.semilogy(loss, label='train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the evolution of the loss\n",
    "\n",
    "# load last checkpoint\n",
    "dctnet1 = torch.load('trainings/tiny_DnCNN_1000.pt')\n",
    "\n",
    "plt.semilogy(dctnet1[3], '.-', label='net1 val')\n",
    "plt.semilogy(dctnet1[2], '.-', label='net1 train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolution of the denoising performance during training\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "# load an image\n",
    "img_clean = io.imread('datasets/BSD68/test002.png', dtype='float32')\n",
    "img_noisy = img_clean + np.random.normal(0, sigma, img_clean.shape)\n",
    "\n",
    "outs = []\n",
    "labels = []\n",
    "\n",
    "# add noisy image\n",
    "outs.append(np.array(img_noisy).clip(0,255))\n",
    "labels.append('noisy')\n",
    "\n",
    "# add clean image\n",
    "outs.append(np.array(img_clean).clip(0,255))\n",
    "labels.append('clean')\n",
    "\n",
    "# add results of iterations\n",
    "for i in range(0,1001,250):\n",
    "    net = torch.load('trainings/tiny_DnCNN_%04d.pt' % i)[0]\n",
    "    out = test_denoiser(net, img_noisy, sigma, has_noise=True)[0]\n",
    "    outs.append(np.array(out).clip(0,255))\n",
    "    labels.append('trained dct net - it %d - %f (dB)' % (i, PSNR(out, img_clean)))\n",
    "\n",
    "\n",
    "# add result of original dct\n",
    "original_dct = DCTlike(8, sigma, initializeDCT=True)\n",
    "out = test_denoiser(original_dct, img_noisy, sigma, has_noise=True)[0]\n",
    "outs.append(np.array(out).clip(0,255))\n",
    "labels.append('original dct - %f (dB)' % (PSNR(out, img_clean)))\n",
    "\n",
    "\n",
    "vistools.display_gallery(outs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "[//]: # (© 2018 Gabriele Facciolo and Pablo Arias)\n",
    "[//]: # (<div style=\"text-align:center; font-size:75%;\"> Copyright © 2018 Gabriele Facciolo and Pablo Arias. All rights reserved.</div> )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
